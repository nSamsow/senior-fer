import torch
import timm
import torch.nn.functional as F
import torch.nn as nn
from timm.models import resnet

from models.cbam import CBAM


def _weight_init(m):
    if isinstance(m, nn.Conv2d):
        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
        if m.bias is not None:
            nn.init.zeros_(m.bias)
    elif isinstance(m, nn.BatchNorm2d):
        nn.init.constant_(m.weight, 1.)
        nn.init.zeros_(m.bias)
    elif isinstance(m, nn.Linear):
        nn.init.normal_(m.weight, 0., .01)
        if m.bias is not None:
            nn.init.zeros_(m.bias)


class My_ResNet50_Ox(nn.Module):
    def __init__(self, num_classes=8, zero_init_last_bn=True, use_cbam=False, drop_rate=0):
        super(My_ResNet50_Ox, self).__init__()
        
        self.drop_rate = drop_rate
        self.relu = nn.ReLU(inplace=True)
        
        self.conv1_7x7_s2 = nn.Conv2d(3, 64, kernel_size=[7, 7], stride=(2, 2), padding=(3, 3), bias=False)
        self.conv1_7x7_s2_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv1_relu_7x7_s2 = nn.ReLU()
        self.pool1_3x3_s2 = nn.MaxPool2d(kernel_size=[3, 3], stride=[2, 2], padding=(0, 0), dilation=1, ceil_mode=True)
        self.conv2_1_1x1_reduce = nn.Conv2d(64, 64, kernel_size=[1, 1], stride=(1, 1), bias=False)
        self.conv2_1_1x1_reduce_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv2_1_1x1_reduce_relu = nn.ReLU()
        self.conv2_1_3x3 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)
        self.conv2_1_3x3_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv2_1_3x3_relu = nn.ReLU()
        self.conv2_1_1x1_increase = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)
        self.conv2_1_1x1_increase_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv2_1_1x1_proj = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)
        self.conv2_1_1x1_proj_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv2_1_relu = nn.ReLU()
        self.conv2_2_1x1_reduce = nn.Conv2d(256, 64, kernel_size=[1, 1], stride=(1, 1), bias=False)
        self.conv2_2_1x1_reduce_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv2_2_1x1_reduce_relu = nn.ReLU()
        self.conv2_2_3x3 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)
        self.conv2_2_3x3_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv2_2_3x3_relu = nn.ReLU()
        self.conv2_2_1x1_increase = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)
        self.conv2_2_1x1_increase_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv2_2_relu = nn.ReLU()
        self.conv2_3_1x1_reduce = nn.Conv2d(256, 64, kernel_size=[1, 1], stride=(1, 1), bias=False)
        self.conv2_3_1x1_reduce_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv2_3_1x1_reduce_relu = nn.ReLU()
        self.conv2_3_3x3 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)
        self.conv2_3_3x3_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv2_3_3x3_relu = nn.ReLU()
        self.conv2_3_1x1_increase = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)
        self.conv2_3_1x1_increase_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv2_3_relu = nn.ReLU()
        self.conv3_1_1x1_reduce = nn.Conv2d(256, 128, kernel_size=[1, 1], stride=(2, 2), bias=False)
        self.conv3_1_1x1_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv3_1_1x1_reduce_relu = nn.ReLU()
        self.conv3_1_3x3 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)
        self.conv3_1_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv3_1_3x3_relu = nn.ReLU()
        self.conv3_1_1x1_increase = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)
        self.conv3_1_1x1_increase_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv3_1_1x1_proj = nn.Conv2d(256, 512, kernel_size=[1, 1], stride=(2, 2), bias=False)
        self.conv3_1_1x1_proj_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv3_1_relu = nn.ReLU()
        self.conv3_2_1x1_reduce = nn.Conv2d(512, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)
        self.conv3_2_1x1_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv3_2_1x1_reduce_relu = nn.ReLU()
        self.conv3_2_3x3 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)
        self.conv3_2_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv3_2_3x3_relu = nn.ReLU()
        self.conv3_2_1x1_increase = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)
        self.conv3_2_1x1_increase_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv3_2_relu = nn.ReLU()
        self.conv3_3_1x1_reduce = nn.Conv2d(512, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)
        self.conv3_3_1x1_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv3_3_1x1_reduce_relu = nn.ReLU()
        self.conv3_3_3x3 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)
        self.conv3_3_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv3_3_3x3_relu = nn.ReLU()
        self.conv3_3_1x1_increase = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)
        self.conv3_3_1x1_increase_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv3_3_relu = nn.ReLU()
        self.conv3_4_1x1_reduce = nn.Conv2d(512, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)
        self.conv3_4_1x1_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv3_4_1x1_reduce_relu = nn.ReLU()
        self.conv3_4_3x3 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)
        self.conv3_4_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv3_4_3x3_relu = nn.ReLU()
        self.conv3_4_1x1_increase = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)
        self.conv3_4_1x1_increase_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv3_4_relu = nn.ReLU()
        self.conv4_1_1x1_reduce = nn.Conv2d(512, 256, kernel_size=[1, 1], stride=(2, 2), bias=False)
        self.conv4_1_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv4_1_1x1_reduce_relu = nn.ReLU()
        self.conv4_1_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)
        self.conv4_1_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv4_1_3x3_relu = nn.ReLU()
        self.conv4_1_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)
        self.conv4_1_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv4_1_1x1_proj = nn.Conv2d(512, 1024, kernel_size=[1, 1], stride=(2, 2), bias=False)
        self.conv4_1_1x1_proj_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv4_1_relu = nn.ReLU()
        self.conv4_2_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)
        self.conv4_2_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv4_2_1x1_reduce_relu = nn.ReLU()
        self.conv4_2_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)
        self.conv4_2_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv4_2_3x3_relu = nn.ReLU()
        self.conv4_2_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)
        self.conv4_2_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv4_2_relu = nn.ReLU()
        self.conv4_3_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)
        self.conv4_3_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv4_3_1x1_reduce_relu = nn.ReLU()
        self.conv4_3_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)
        self.conv4_3_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv4_3_3x3_relu = nn.ReLU()
        self.conv4_3_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)
        self.conv4_3_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv4_3_relu = nn.ReLU()
        self.conv4_4_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)
        self.conv4_4_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv4_4_1x1_reduce_relu = nn.ReLU()
        self.conv4_4_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)
        self.conv4_4_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv4_4_3x3_relu = nn.ReLU()
        self.conv4_4_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)
        self.conv4_4_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv4_4_relu = nn.ReLU()
        self.conv4_5_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)
        self.conv4_5_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv4_5_1x1_reduce_relu = nn.ReLU()
        self.conv4_5_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)
        self.conv4_5_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv4_5_3x3_relu = nn.ReLU()
        self.conv4_5_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)
        self.conv4_5_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv4_5_relu = nn.ReLU()
        self.conv4_6_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)
        self.conv4_6_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv4_6_1x1_reduce_relu = nn.ReLU()
        self.conv4_6_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)
        self.conv4_6_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv4_6_3x3_relu = nn.ReLU()
        self.conv4_6_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)
        self.conv4_6_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv4_6_relu = nn.ReLU()
        self.conv5_1_1x1_reduce = nn.Conv2d(1024, 512, kernel_size=[1, 1], stride=(2, 2), bias=False)
        self.conv5_1_1x1_reduce_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv5_1_1x1_reduce_relu = nn.ReLU()
        self.conv5_1_3x3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)
        self.conv5_1_3x3_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv5_1_3x3_relu = nn.ReLU()
        self.conv5_1_1x1_increase = nn.Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), bias=False)
        self.conv5_1_1x1_increase_bn = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv5_1_1x1_proj = nn.Conv2d(1024, 2048, kernel_size=[1, 1], stride=(2, 2), bias=False)
        self.conv5_1_1x1_proj_bn = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv5_1_relu = nn.ReLU()
        self.conv5_2_1x1_reduce = nn.Conv2d(2048, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)
        self.conv5_2_1x1_reduce_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv5_2_1x1_reduce_relu = nn.ReLU()
        self.conv5_2_3x3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)
        self.conv5_2_3x3_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv5_2_3x3_relu = nn.ReLU()
        self.conv5_2_1x1_increase = nn.Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), bias=False)
        self.conv5_2_1x1_increase_bn = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv5_2_relu = nn.ReLU()
        self.conv5_3_1x1_reduce = nn.Conv2d(2048, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)
        self.conv5_3_1x1_reduce_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv5_3_1x1_reduce_relu = nn.ReLU()
        self.conv5_3_3x3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)
        self.conv5_3_3x3_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv5_3_3x3_relu = nn.ReLU()
        self.conv5_3_1x1_increase = nn.Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), bias=False)
        self.conv5_3_1x1_increase_bn = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.conv5_3_relu = nn.ReLU()
        self.pool5_7x7_s1 = nn.AvgPool2d(kernel_size=[7, 7], stride=[1, 1], padding=0)
        self.classifier = nn.Conv2d(2048, 8631, kernel_size=[1, 1], stride=(1, 1))
        
        # Additional Modules
        self.num_features = 2048
        self.cbam = CBAM(self.num_features, reduction_ratio=16, bn=False) if use_cbam else None
        self.global_pool = nn.AdaptiveAvgPool2d(1)
        self.alpha = nn.Sequential(nn.Linear(self.num_features, 1, bias=False), nn.Sigmoid())
        self.beta = nn.Sequential(nn.Linear(self.num_features * 2, 1, bias=False), nn.Sigmoid())
        # self.classifier = nn.Linear(self.num_features * 2, num_classes)
        
        # Weight Initialization
        for m in self.modules():
            _weight_init(m)
        if zero_init_last_bn:
            for m in self.modules():
                if hasattr(m, 'zero_init_last_bn'):
                    m.zero_init_last_bn()
    
    def forward(self, x):
        features = []
        alphas = []
        for i in range(6):
            f = x[:, :, :, :, i]

            f = self.conv1_7x7_s2(f)
            f = self.conv1_7x7_s2_bn(f)
            f = self.relu(f)
            pool1_3x3_s2 = self.pool1_3x3_s2(f)
            
            f = self.conv2_1_1x1_reduce(pool1_3x3_s2)
            f = self.conv2_1_1x1_reduce_bn(f)
            f = self.relu(f)
            f = self.conv2_1_3x3(f)
            f = self.conv2_1_3x3_bn(f)
            f = self.relu(f)
            f = self.conv2_1_1x1_increase(f)
            f = self.conv2_1_1x1_increase_bn(f)
            conv2_1_1x1_proj = self.conv2_1_1x1_proj(pool1_3x3_s2)
            conv2_1_1x1_proj_bn = self.conv2_1_1x1_proj_bn(conv2_1_1x1_proj)
            f = torch.add(conv2_1_1x1_proj_bn, 1, f)
            conv2_1x = self.relu(f)
            f = self.conv2_2_1x1_reduce(conv2_1x)
            f = self.conv2_2_1x1_reduce_bn(f)
            f = self.relu(f)
            f = self.conv2_2_3x3(f)
            f = self.conv2_2_3x3_bn(f)
            f = self.relu(f)
            f = self.conv2_2_1x1_increase(f)
            f = self.conv2_2_1x1_increase_bn(f)
            f = torch.add(conv2_1x, 1, f)
            conv2_2x = self.relu(f)
            f = self.conv2_3_1x1_reduce(conv2_2x)
            f = self.conv2_3_1x1_reduce_bn(f)
            f = self.relu(f)
            f = self.conv2_3_3x3(f)
            f = self.conv2_3_3x3_bn(f)
            f = self.relu(f)
            f = self.conv2_3_1x1_increase(f)
            f = self.conv2_3_1x1_increase_bn(f)
            f = torch.add(conv2_2x, 1, f)
            conv2_3x = self.relu(f)
            
            f = self.conv3_1_1x1_reduce(conv2_3x)
            f = self.conv3_1_1x1_reduce_bn(f)
            f = self.relu(f)
            f = self.conv3_1_3x3(f)
            f = self.conv3_1_3x3_bn(f)
            f = self.relu(f)
            f = self.conv3_1_1x1_increase(f)
            f = self.conv3_1_1x1_increase_bn(f)
            conv3_1_1x1_proj = self.conv3_1_1x1_proj(conv2_3x)
            conv3_1_1x1_proj_bn = self.conv3_1_1x1_proj_bn(conv3_1_1x1_proj)
            f = torch.add(conv3_1_1x1_proj_bn, 1, f)
            conv3_1x = self.relu(f)
            f = self.conv3_2_1x1_reduce(conv3_1x)
            f = self.conv3_2_1x1_reduce_bn(f)
            f = self.relu(f)
            f = self.conv3_2_3x3(f)
            f = self.conv3_2_3x3_bn(f)
            f = self.relu(f)
            f = self.conv3_2_1x1_increase(f)
            f = self.conv3_2_1x1_increase_bn(f)
            f = torch.add(conv3_1x, 1, f)
            conv3_2x = self.relu(f)
            f = self.conv3_3_1x1_reduce(conv3_2x)
            f = self.conv3_3_1x1_reduce_bn(f)
            f = self.relu(f)
            f = self.conv3_3_3x3(f)
            f = self.conv3_3_3x3_bn(f)
            f = self.relu(f)
            f = self.conv3_3_1x1_increase(f)
            f = self.conv3_3_1x1_increase_bn(f)
            f = torch.add(conv3_2x, 1, f)
            conv3_3x = self.relu(f)
            f = self.conv3_4_1x1_reduce(conv3_3x)
            f = self.conv3_4_1x1_reduce_bn(f)
            f = self.relu(f)
            f = self.conv3_4_3x3(f)
            f = self.conv3_4_3x3_bn(f)
            f = self.relu(f)
            f = self.conv3_4_1x1_increase(f)
            f = self.conv3_4_1x1_increase_bn(f)
            f = torch.add(conv3_3x, 1, f)
            conv3_4x = self.relu(f)
            
            f = self.conv4_1_1x1_reduce(conv3_4x)
            f = self.conv4_1_1x1_reduce_bn(f)
            f = self.relu(f)
            f = self.conv4_1_3x3(f)
            f = self.conv4_1_3x3_bn(f)
            f = self.relu(f)
            f = self.conv4_1_1x1_increase(f)
            f = self.conv4_1_1x1_increase_bn(f)
            conv4_1_1x1_proj = self.conv4_1_1x1_proj(conv3_4x)
            conv4_1_1x1_proj_bn = self.conv4_1_1x1_proj_bn(conv4_1_1x1_proj)
            f = torch.add(conv4_1_1x1_proj_bn, 1, f)
            conv4_1x = self.relu(f)
            f = self.conv4_2_1x1_reduce(conv4_1x)
            f = self.conv4_2_1x1_reduce_bn(f)
            f = self.relu(f)
            f = self.conv4_2_3x3(f)
            f = self.conv4_2_3x3_bn(f)
            f = self.relu(f)
            f = self.conv4_2_1x1_increase(f)
            f = self.conv4_2_1x1_increase_bn(f)
            f = torch.add(conv4_1x, 1, f)
            conv4_2x = self.relu(f)
            f = self.conv4_3_1x1_reduce(conv4_2x)
            f = self.conv4_3_1x1_reduce_bn(f)
            f = self.relu(f)
            f = self.conv4_3_3x3(f)
            f = self.conv4_3_3x3_bn(f)
            f = self.relu(f)
            f = self.conv4_3_1x1_increase(f)
            f = self.conv4_3_1x1_increase_bn(f)
            f = torch.add(conv4_2x, 1, f)
            conv4_3x = self.relu(f)
            f = self.conv4_4_1x1_reduce(conv4_3x)
            f = self.conv4_4_1x1_reduce_bn(f)
            f = self.relu(f)
            f = self.conv4_4_3x3(f)
            f = self.conv4_4_3x3_bn(f)
            f = self.relu(f)
            f = self.conv4_4_1x1_increase(f)
            f = self.conv4_4_1x1_increase_bn(f)
            f = torch.add(conv4_3x, 1, f)
            conv4_4x = self.relu(f)
            f = self.conv4_5_1x1_reduce(conv4_4x)
            f = self.conv4_5_1x1_reduce_bn(f)
            f = self.relu(f)
            f = self.conv4_5_3x3(f)
            f = self.conv4_5_3x3_bn(f)
            f = self.relu(f)
            f = self.conv4_5_1x1_increase(f)
            f = self.conv4_5_1x1_increase_bn(f)
            f = torch.add(conv4_4x, 1, f)
            conv4_5x = self.relu(f)
            f = self.conv4_6_1x1_reduce(conv4_5x)
            f = self.conv4_6_1x1_reduce_bn(f)
            f = self.relu(f)
            f = self.conv4_6_3x3(f)
            f = self.conv4_6_3x3_bn(f)
            f = self.relu(f)
            f = self.conv4_6_1x1_increase(f)
            f = self.conv4_6_1x1_increase_bn(f)
            f = torch.add(conv4_5x, 1, f)
            conv4_6x = self.relu(f)
            
            f = self.conv5_1_1x1_reduce(conv4_6x)
            f = self.conv5_1_1x1_reduce_bn(f)
            f = self.relu(f)
            f = self.conv5_1_3x3(f)
            f = self.conv5_1_3x3_bn(f)
            f = self.relu(f)
            f = self.conv5_1_1x1_increase(f)
            f = self.conv5_1_1x1_increase_bn(f)
            conv5_1_1x1_proj = self.conv5_1_1x1_proj(conv4_6x)
            conv5_1_1x1_proj_bn = self.conv5_1_1x1_proj_bn(conv5_1_1x1_proj)
            f = torch.add(conv5_1_1x1_proj_bn, 1, f)
            conv5_1x = self.relu(f)
            f = self.conv5_2_1x1_reduce(conv5_1x)
            f = self.conv5_2_1x1_reduce_bn(f)
            f = self.relu(f)
            f = self.conv5_2_3x3(f)
            f = self.conv5_2_3x3_bn(f)
            f = self.relu(f)
            f = self.conv5_2_1x1_increase(f)
            f = self.conv5_2_1x1_increase_bn(f)
            f = torch.add(conv5_1x, 1, f)
            conv5_2x = self.relu(f)
            f = self.conv5_3_1x1_reduce(conv5_2x)
            f = self.conv5_3_1x1_reduce_bn(f)
            f = self.relu(f)
            f = self.conv5_3_3x3(f)
            f = self.conv5_3_3x3_bn(f)
            f = self.relu(f)
            f = self.conv5_3_1x1_increase(f)
            f = self.conv5_3_1x1_increase_bn(f)
            f = torch.add(conv5_2x, 1, f)
            f = self.relu(f)
            # f = self.pool5_7x7_s1(conv5_3x)
            # classifier_preflatten = self.classifier(pool5_7x7_s1)
            # classifier = classifier_preflatten.view(classifier_preflatten.size(0), -1)

            if self.cbam is not None:
                f = self.cbam(f)
            
            f = self.global_pool(f)
            f = f.view(f.size(0), -1)

            features.append(f)
            alphas.append(self.alpha(f))

        # Self-attention Module
        feature_stack = torch.stack(features, dim=2)
        alpha_stack = torch.stack(alphas, dim=2)
        alpha_stack = F.softmax(alpha_stack, dim=2)
        alpha_orig = alpha_stack[:, :, 0]
        alpha_part_max = alpha_stack[:, :, 1:].max(dim=2)[0]
        Fm = feature_stack.mul(alpha_stack).sum(2).div(alpha_stack.sum(2))
        
        # Relation-attention Module
        betas = []
        for i in range(len(features)):
            features[i] = torch.cat([features[i], Fm], dim=1)
            betas.append(self.beta(features[i]))
        
        feature_stack = torch.stack(features, dim=2)
        beta_stack = torch.stack(betas, dim=2)
        beta_stack = F.softmax(beta_stack, dim=2)
        output = (feature_stack.mul(beta_stack * alpha_stack)
                  .sum(2)
                  .div((beta_stack * alpha_stack).sum(2)))
        output = output.view(output.size(0), -1)
        
        if self.drop_rate > 0.:
            output = F.dropout(output, p=self.drop_rate, training=self.training)
        
        pred_score = self.classifier(output)
        
        return pred_score, alpha_part_max, alpha_orig
